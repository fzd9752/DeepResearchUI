# DeepResearch Environment Configuration
# Copy this file to .env and fill in your actual values
# Run: cp .env.example .env

# =============================================================================
# Model and Inference Hyperparameters
# =============================================================================
# Main-agent LLM Model Selection
LLM_MODEL=deepseek-ai/DeepSeek-V3.2

# MODEL_PATH: Path to the local model directory for counting tokens
MODEL_PATH=/your/model/path

# DATASET: Path to the input dataset (JSONL format)
DATASET=your_dataset_name

# FILE_ROOT_PATH: Root path for file-based tasks
FILE_ROOT_PATH=/your/file/root/path

# OUTPUT_PATH: Path to save the output results
OUTPUT_PATH=/your/output/path

# Inference Hyperparameters
ROLLOUT_COUNT=3
TEMPERATURE=0.85
PRESENCE_PENALTY=1.1
MAX_WORKERS=30

# =============================================================================
# API Keys and External Services
# =============================================================================
# Serper API for web search and Google Scholar
# Get your key from: https://serper.dev/
SERPER_KEY_ID=your_key

# Jina API for web page reading
# Get your key from: https://jina.ai/
JINA_API_KEYS=your_key

# Agent API (OpenAI-compatible) 
AGENT_API_KEY=your_venus_api_key
AGENT_API_BASE=https://api.gmi-serving.com/v1

# Summary model API (OpenAI-compatible) for page summarization
# Get your key from: https://platform.openai.com/
SUMMARY_MODEL_NAME=deepseek-ai/DeepSeek-V3.2
SUMMARY_API_KEY=your_key
SUMMARY_API_BASE=https://api.gmi-serving.com/v1

# =============================================================================
# Python Code Execution Sandbox
# =============================================================================

# SandboxFusion endpoints for Python interpreter
# Example: "http://22.16.67.220:8080,http://22.16.78.153:8080,http://22.17.10.216:8080"
# See: https://github.com/bytedance/SandboxFusion
SANDBOX_FUSION_ENDPOINT=your_sandbox_endpoint
TORCH_COMPILE_CACHE_DIR=./cache

# =============================================================================
# Browser Agent (BrowserUse MCP) - Optional
# =============================================================================
# If you don't set these variables, code will use built-in defaults.
# ENABLE_BROWSER_AGENT: Whether to enable the BrowserUse browser agent.
# - true/false values supported: 1/0, true/false, yes/no, y/n, on/off
ENABLE_BROWSER_AGENT=true

# BROWSER_AGENT_ENDPOINT: BrowserUse MCP server endpoint
# Default (if not set): http://127.0.0.1:9091/browser_use_agent
BROWSER_AGENT_ENDPOINT=http://127.0.0.1:9091/browser_use_agent

# BROWSER_AGENT_MAX_STEPS: Max exploration steps for browser agent
# Default (if not set): 10
BROWSER_AGENT_MAX_STEPS=10

# VISIT_SERVER_TIMEOUT: Shared overall visit timeout (seconds)
# Default (if not set): 200
VISIT_SERVER_TIMEOUT=200

# BROWSER_AGENT_TIMEOUT: Browser agent request timeout (seconds)
# Default (if not set): falls back to VISIT_SERVER_TIMEOUT, then 200
BROWSER_AGENT_TIMEOUT=200

# =============================================================================
# Deep Analysis Agent - Optional
# =============================================================================
# Enable deep analysis agent for processing file URLs (zip, csv, txt)
ENABLE_DEEP_ANALYSIS=true

# =============================================================================
# Reflection and Fallback Control - Optional
# =============================================================================
# Enable reflection prompts when model fails to respond properly
# Set to 1/true/yes to enable, 0/false/no to disable (default: enabled)
ENABLE_REFLECTION=true

# =============================================================================
# Context Management - Optional
# =============================================================================
# Whether to enable the dynamic context management module.
# Set to 1/true/yes to enable, 0/false/no to disable (default: enabled)
ENABLE_CONTEXT_MANAGEMENT=true
# Context LLM (OpenAI-compatible) 
MEMORY_MODEL=deepseek-ai/DeepSeek-V3.2
MEMORY_API_KEY=your_key_id
MEMORY_API_BASE=https://api.gmi-serving.com/v1
MEMORY_TEMPERATURE=0.8

# =============================================================================
# API Server - Optional
# =============================================================================
# Stream stdout/stderr to SSE as debug_log events (default: false)
API_DEBUG_STREAM=false
# Include full tool responses in SSE round_observing payloads (default: false)
API_FULL_TOOL_RESPONSE=false
# Debug log directory for stdout/stderr (default: ./logs/debug)
API_DEBUG_LOG_DIR=./logs/debug

# =============================================================================
# OCR (Optional)
# =============================================================================
# OCR language for tesseract (default: chi_sim+eng)
TESSERACT_LANG=chi_sim+eng
